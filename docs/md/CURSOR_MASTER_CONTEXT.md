# ðŸš€ CURSOR MASTER CONTEXT - Paulo Sousa Career Accelerator

> **DOCUMENTO MASTER**: Todo o contexto do projeto de transiÃ§Ã£o de carreira em Dados + IA  
> **Projeto**: paulo-career-accelerator  
> **LocalizaÃ§Ã£o**: C:\projetos\paulo_sousa\paulo-career-accelerator  
> **Data**: Novembro 2024  
> **Objetivo**: Portfolio completo em 4 dias para reposicionamento profissional

---

## ðŸ“‹ ÃNDICE RÃPIDO

1. [CONTEXTO DO PROFISSIONAL](#contexto-profissional)
2. [OBJETIVO E ESTRATÃ‰GIA](#objetivo-estrategia)
3. [ESTRUTURA DO PROJETO](#estrutura-projeto)
4. [CRONOGRAMA 4 DIAS](#cronograma)
5. [PROJETOS A IMPLEMENTAR](#projetos)
6. [PADRÃ•ES E CONVENÃ‡Ã•ES](#padroes)
7. [FLUXO DE TRABALHO](#fluxo-trabalho)
8. [COMANDOS E SNIPPETS](#comandos)
9. [RECURSOS E LINKS](#recursos)

---

## ðŸ‘¤ CONTEXTO PROFISSIONAL {#contexto-profissional}

### **Paulo Cesar Mendes de Sousa Junior**

**LocalizaÃ§Ã£o**: BrasÃ­lia-DF, Brasil

**SituaÃ§Ã£o Atual**:
- Desempregado (urgÃªncia reposicionamento)
- 25+ anos experiÃªncia em dados
- Necessidade transiÃ§Ã£o para mercado Dados + IA
- Prazo: 30-60 dias para novas oportunidades
- MotivaÃ§Ã£o: alta, comprometido com sprint intensivo

**Background Profissional**:
- **ExperiÃªncia SÃªnior**: TCU (Tribunal de Contas da UniÃ£o), TST (Tribunal Superior do Trabalho), PRF, Sebrae-BA, tribunais federais
- **Atual**: Logiks Tecnologias (desde 2019) - Senior Data Analyst
- **EspecializaÃ§Ã£o**: Data Warehouse, Business Intelligence, Data Governance (DMBOK desde 2009), Data Engineering
- **Diferencial Ãšnico**: ExperiÃªncia profunda com dados governamentais brasileiros + compliance + LGPD

**Stack TÃ©cnico Atual**:

**DomÃ­nio AvanÃ§ado**:
- SQL (25 anos, expert)
- Modelagem dimensional (Star Schema, Snowflake)
- Data Warehouse architecture
- dbt (data build tool) - transformaÃ§Ãµes, macros, testes
- Power BI (dashboards, DAX, Power Query)
- SAP BusinessObjects (legado)
- SAP PowerDesigner (modelagem)
- Python para data engineering (pandas, SQLAlchemy)
- SAS (legado)
- Trino (distributed SQL)

**Stack Moderno em Desenvolvimento**:
- IA Generativa: iniciando (OpenAI API, LangChain)
- RAG (Retrieval-Augmented Generation): aprendendo
- Vector Databases (Chroma, Pinecone): novo
- MLOps: bÃ¡sico (MLflow, Great Expectations)
- Cloud: GCP bÃ¡sico, Azure bÃ¡sico

**Metodologias e Frameworks**:
- DMBOK (Data Management Body of Knowledge) - expert
- LGPD (Lei Geral de ProteÃ§Ã£o de Dados) - compliance
- Data Governance - polÃ­ticas, processos, auditoria
- DocumentaÃ§Ã£o tÃ©cnica - forte

**Produto ProprietÃ¡rio**:
- **"Jornada de Dados"**: metodologia de data discovery e reverse engineering para clientes como Furnas (EletrobrÃ¡s)
- EspecializaÃ§Ã£o em reverse engineering de sistemas sem documentaÃ§Ã£o

**Gap Analysis**:
- âœ… **Dominado**: SQL, DW, BI, GovernanÃ§a, dbt, Python data
- ðŸ”„ **Em Desenvolvimento**: Prompt Engineering, LangChain, RAG, Vector DBs
- âŒ **Gaps CrÃ­ticos**: MLOps produÃ§Ã£o, Fine-tuning, Real-time (Kafka), Data Mesh, K8s/Docker avanÃ§ado

---

## ðŸŽ¯ OBJETIVO E ESTRATÃ‰GIA {#objetivo-estrategia}

### **Objetivo PrimÃ¡rio**

**Reposicionamento Profissional de:**
```
Data Engineer Tradicional
        â†“
Data Architect + AI Engineer
```

**Posicionamento Ãšnico**:
> "T-Shaped AI-Augmented Data Professional: 25 anos de experiÃªncia em dados + expertise em IA Generativa = soluÃ§Ãµes modernas para problemas reais em ambientes regulados"

### **Diferencial Competitivo**

1. **ExperiÃªncia Rara**: 25 anos dados + Ã³rgÃ£os pÃºblicos + LGPD/compliance + IA
2. **Implementador**: entrega projetos funcionando, nÃ£o sÃ³ teoria
3. **Bridge Builder**: conecta sistemas legados com arquitetura moderna
4. **Contexto de NegÃ³cio**: entende complexidade organizacional
5. **Documentador**: cria documentaÃ§Ã£o que equipes realmente usam

### **EstratÃ©gia de TransiÃ§Ã£o**

**Abordagem**: Sprint intensivo de 4 dias para criar portfolio tÃ©cnico robusto que demonstre capacidade de aplicar IA Generativa a problemas reais de dados.

**Nichos de Mercado Identificados**:

1. **IA Aplicada a GovernanÃ§a de Dados** â­ (Nicho mais quente)
   - ClassificaÃ§Ã£o automÃ¡tica de dados sensÃ­veis (LGPD)
   - Data Quality com ML para detecÃ§Ã£o de anomalias
   - Lineage automÃ¡tico usando LLMs
   - Auditorias automatizadas de conformidade
   - Clientes-alvo: Bancos, seguradoras, Ã³rgÃ£os pÃºblicos, healthtechs
   - Faixa: R$ 200-400/hora consultoria

2. **ModernizaÃ§Ã£o de Sistemas Legados com IA**
   - MigraÃ§Ã£o BI legado (SAP BO, Cognos) para moderno (dbt, Looker)
   - Reverse engineering de ETLs com LLMs
   - DocumentaÃ§Ã£o automÃ¡tica de sistemas sem documentaÃ§Ã£o
   - TraduÃ§Ã£o cÃ³digo SAS/Informatica para Python/SQL
   - Clientes: Grandes corporaÃ§Ãµes, governo federal
   - Projetos: R$ 80k-150k (3-6 meses)

3. **BI Aumentado (Augmented Analytics)**
   - Analytics conversacional com Q&A em linguagem natural
   - Alertas inteligentes e insights automÃ¡ticos
   - Narrativas automÃ¡ticas com GPT
   - Chat com dados corporativos (RAG sobre Data Warehouse)
   - PosiÃ§Ãµes: Head of Analytics, BI + AI Lead, CDO

4. **MLOps e DataOps para Governo**
   - Pipelines ML para Ã³rgÃ£os pÃºblicos
   - Arquiteturas Data Platform para Smart Cities
   - IA para otimizaÃ§Ã£o de recursos pÃºblicos
   - Vantagem: networking e expertise em compliance setor pÃºblico
   - Faixa: R$ 18k-30k CLT, R$ 250-350/hora PJ

**PosiÃ§Ãµes-Alvo**:
- **Tier 1**: Senior Data Architect, Lead Data Engineer, Principal Data Engineer, Head of Data Governance, CDO (empresas mÃ©dias)
- **Tier 2**: Senior Analytics Engineer, Data Platform Engineer, MLOps Engineer, Senior Data Consultant
- **Faixa Salarial CLT**: R$ 15k-60k (dependendo nÃ­vel/empresa)
- **Faixa PJ/Consultoria**: R$ 200-400/hora, projetos R$ 80k-150k

### **Elevator Pitch**

```
"Arquiteto de dados com 25 anos transformando dados em valor para 
organizaÃ§Ãµes complexas como TCU e Sebrae. Especializado em modernizar 
sistemas legados usando IA - desde governanÃ§a automatizada atÃ© analytics 
conversacional. Combino experiÃªncia profunda com Ã³rgÃ£os pÃºblicos brasileiros 
e expertise tÃ©cnica em dbt, Python, e IA Generativa para entregar soluÃ§Ãµes 
que funcionam em ambientes altamente regulados."
```

---

## ðŸ“ ESTRUTURA DO PROJETO {#estrutura-projeto}

### **Estrutura de Pastas Atual**

```
C:\projetos\paulo_sousa\paulo-career-accelerator\
â”‚
â”œâ”€â”€ docs\
â”‚   â”œâ”€â”€ md\                                      # DocumentaÃ§Ã£o em Markdown
â”‚   â”‚   â”œâ”€â”€ PLANO_COMPLETO_TRANSICAO_CARREIRA.md
â”‚   â”‚   â”œâ”€â”€ GUIA_PASSO_A_PASSO.md
â”‚   â”‚   â””â”€â”€ CURSOR_CONTEXT_CAREER_TRANSITION.md
â”‚   â”‚
â”‚   â”œâ”€â”€ posts\                                   # Posts LinkedIn
â”‚   â””â”€â”€ templates\                               # Templates diversos
â”‚
â”œâ”€â”€ projects\                                    # ðŸŽ¯ PROJETOS DO PORTFOLIO
â”‚   â”‚
â”‚   â”œâ”€â”€ 01-database-documentation-assistant\    # Projeto 1
â”‚   â”‚   â”œâ”€â”€ src\
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ database_connector.py
â”‚   â”‚   â”‚   â”œâ”€â”€ schema_extractor.py
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_documenter.py
â”‚   â”‚   â”‚   â”œâ”€â”€ markdown_generator.py
â”‚   â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ app.py                              # Streamlit UI
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ .env.example
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ examples\
â”‚   â”‚   â””â”€â”€ tests\
â”‚   â”‚
â”‚   â”œâ”€â”€ 02-data-lineage-analyzer\               # Projeto 2
â”‚   â”‚   â”œâ”€â”€ src\
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ sql_parser.py
â”‚   â”‚   â”‚   â”œâ”€â”€ dbt_parser.py
â”‚   â”‚   â”‚   â”œâ”€â”€ dependency_extractor.py
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_analyzer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ graph_builder.py
â”‚   â”‚   â”‚   â”œâ”€â”€ visualizer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ impact_analyzer.py
â”‚   â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ .env.example
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â””â”€â”€ examples\
â”‚   â”‚
â”‚   â””â”€â”€ 03-rag-data-dictionary\                 # Projeto 3
â”‚       â”œâ”€â”€ src\
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ document_loader.py
â”‚       â”‚   â”œâ”€â”€ chunking.py
â”‚       â”‚   â”œâ”€â”€ embedding_generator.py
â”‚       â”‚   â”œâ”€â”€ vector_store.py
â”‚       â”‚   â”œâ”€â”€ rag_chain.py
â”‚       â”‚   â”œâ”€â”€ chat_interface.py
â”‚       â”‚   â”œâ”€â”€ feedback_logger.py
â”‚       â”‚   â””â”€â”€ config.py
â”‚       â”œâ”€â”€ app.py
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â”œâ”€â”€ .env.example
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ data\                               # Sample data
â”‚       â””â”€â”€ vector_db\                          # Vector database
â”‚
â”œâ”€â”€ resources\                                   # Recursos de aprendizado
â”‚   â”œâ”€â”€ courses\
â”‚   â”œâ”€â”€ tutorials\
â”‚   â””â”€â”€ references\
â”‚
â”œâ”€â”€ scripts\                                     # Scripts Ãºteis
â”‚   â”œâ”€â”€ setup_environment.bat
â”‚   â”œâ”€â”€ run_project_1.bat
â”‚   â”œâ”€â”€ run_project_2.bat
â”‚   â””â”€â”€ run_project_3.bat
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                                    # README principal
â””â”€â”€ STARTED.txt                                  # Log de inÃ­cio
```

### **Arquivos Importantes**

**DocumentaÃ§Ã£o Master**:
- `docs/md/PLANO_COMPLETO_TRANSICAO_CARREIRA.md` - Planejamento completo
- `docs/md/GUIA_PASSO_A_PASSO.md` - Tutorial passo a passo
- `docs/md/CURSOR_CONTEXT_CAREER_TRANSITION.md` - Contexto original para Cursor

**Este Arquivo**:
- `CURSOR_MASTER_CONTEXT.md` - Documento master para Cursor Desktop

---

## ðŸ“… CRONOGRAMA 4 DIAS {#cronograma}

### **Metodologia: Sprint Intensivo com Pair Programming (Claude AI)**

**Premissa**: Usar Claude AI como pair programmer para reduzir tempo de desenvolvimento em ~60%

**Tempo Estimado vs Realidade**:
- âŒ **Sem IA**: 42 horas total
- âœ… **Com Claude**: 16-18 horas total (~60% economia)

**ReduÃ§Ã£o Por Projeto**:
- Projeto 1: 8h â†’ 3-4h (50%)
- Projeto 2: 10h â†’ 4-5h (50%)
- Projeto 3: 12h â†’ 5-6h (50%)
- READMEs: 6h â†’ 2h (67%)
- Posts LinkedIn: 2h â†’ 0.5h (75%)
- Outros: 4h â†’ 1.5h (63%)

### **DIA 1 - Segunda-feira (10h trabalho)**

**MANHÃƒ (4h) - Setup e Projeto 1 InÃ­cio**:
```
â° 08:00-09:00 | LinkedIn + Networking Inicial
  [ ] Atualizar perfil LinkedIn (tÃ­tulo, headline, skills)
  [ ] Adicionar #OpenToWork
  [ ] Publicar POST #1 (Reposicionamento)
  [ ] Contatar 5 headhunters (mensagens personalizadas)
  
â° 09:00-10:00 | Setup TÃ©cnico
  [ ] Verificar Python 3.10+ instalado
  [ ] Criar venv no projeto
  [ ] Instalar dependÃªncias base (requirements.txt)
  [ ] Testar OpenAI API key
  [ ] Git init + primeiro commit
  
â° 10:00-12:00 | Projeto 1 - Database Documentation Assistant (50%)
  [ ] Criar estrutura de pastas
  [ ] Implementar database_connector.py (com Claude)
  [ ] Implementar schema_extractor.py (com Claude)
  [ ] Implementar llm_documenter.py (com Claude)
  [ ] Testar com banco de exemplo
```

**TARDE (4h) - Projeto 1 ConclusÃ£o**:
```
â° 13:00-15:00 | Projeto 1 ContinuaÃ§Ã£o (80%)
  [ ] Implementar markdown_generator.py
  [ ] Implementar app.py (Streamlit UI)
  [ ] Testes manuais
  [ ] Screenshots da interface
  
â° 15:00-16:00 | Projeto 1 FinalizaÃ§Ã£o
  [ ] README.md completo (com Claude)
  [ ] .env.example
  [ ] requirements.txt final
  [ ] Git commit + push
  
â° 16:00-17:00 | PublicaÃ§Ã£o e Marketing
  [ ] Demo video (2-3 min, screen recording)
  [ ] Publicar POST #2 LinkedIn (Projeto 1)
  [ ] Atualizar README principal do portfolio
```

**NOITE (2h) - Networking e Estudos**:
```
â° 19:00-20:00 | Networking
  [ ] Contatar 5 headhunters adicionais
  [ ] 5 aplicaÃ§Ãµes de vagas (LinkedIn, Indeed)
  [ ] Responder mensagens/comentÃ¡rios LinkedIn
  
â° 20:00-21:00 | Estudos
  [ ] DeepLearning.AI: "ChatGPT Prompt Engineering" (1h)
  [ ] Anotar aprendizados
```

### **DIA 2 - TerÃ§a-feira (10h trabalho)**

**MANHÃƒ (4h) - Projeto 2 Completo**:
```
â° 08:00-10:00 | Projeto 2 - Data Lineage Analyzer (ImplementaÃ§Ã£o)
  [ ] Criar estrutura de pastas
  [ ] sql_parser.py (com Claude)
  [ ] dbt_parser.py (com Claude)
  [ ] dependency_extractor.py (com Claude)
  [ ] llm_analyzer.py (com Claude)
  
â° 10:00-12:00 | Projeto 2 ContinuaÃ§Ã£o
  [ ] graph_builder.py (NetworkX)
  [ ] visualizer.py (Plotly)
  [ ] impact_analyzer.py
  [ ] app.py (Streamlit UI)
  [ ] Testes com SQL/dbt examples
```

**TARDE (4h) - Projeto 2 FinalizaÃ§Ã£o + CertificaÃ§Ã£o**:
```
â° 13:00-14:00 | Projeto 2 FinalizaÃ§Ã£o
  [ ] README.md completo
  [ ] Screenshots + demo video
  [ ] Git commit + push
  [ ] POST #3 LinkedIn (Projeto 2)
  
â° 14:00-17:00 | Microsoft AI-900 Certification
  [ ] RevisÃ£o material (Microsoft Learn)
  [ ] Fazer prova online
  [ ] Obter certificaÃ§Ã£o
  [ ] Atualizar LinkedIn com badge
```

**NOITE (2h) - Networking e Estudos**:
```
â° 19:00-20:00 | Networking
  [ ] 5 aplicaÃ§Ãµes vagas
  [ ] Contatar 5 headhunters
  [ ] Postar certificaÃ§Ã£o LinkedIn (POST #4)
  
â° 20:00-21:00 | Estudos RAG
  [ ] LangChain documentation (RAG basics)
  [ ] Vector databases overview
```

### **DIA 3 - Quarta-feira (10h trabalho)**

**MANHÃƒ (4h) - Estudos RAG + Projeto 3 InÃ­cio**:
```
â° 08:00-10:00 | Estudos Aprofundados RAG
  [ ] LangChain curso (RAG section)
  [ ] Embeddings e similarity search
  [ ] Chroma/Pinecone tutorials
  
â° 10:00-12:00 | Projeto 3 - RAG Data Dictionary (InÃ­cio)
  [ ] Criar estrutura de pastas
  [ ] document_loader.py (com Claude)
  [ ] chunking.py (com Claude)
  [ ] embedding_generator.py (com Claude)
  [ ] vector_store.py (Chroma local)
```

**TARDE (4h) - Projeto 3 RAG Chain**:
```
â° 13:00-15:00 | Projeto 3 ContinuaÃ§Ã£o
  [ ] rag_chain.py (LangChain + OpenAI)
  [ ] chat_interface.py
  [ ] feedback_logger.py
  
â° 15:00-17:00 | Projeto 3 UI e Testes
  [ ] app.py (Streamlit conversational UI)
  [ ] Adicionar sample data (schemas, dbt docs)
  [ ] Popular vector database
  [ ] Testar queries conversacionais
  [ ] Screenshots + demo video
```

**NOITE (2h) - Networking Intensivo**:
```
â° 19:00-20:00 | Networking AvanÃ§ado
  [ ] Contatar 5 CTOs/Heads of Data (LinkedIn)
  [ ] Participar discussÃµes em comunidades
    - dbt Slack
    - Data Engineering Brasil (Telegram)
    - AI Engineers (Discord)
  
â° 20:00-21:00 | PreparaÃ§Ã£o Artigo
  [ ] Outline artigo tÃ©cnico (Medium)
  [ ] Pesquisar referÃªncias
  [ ] EsboÃ§ar seÃ§Ãµes principais
```

### **DIA 4 - Quinta-feira (8h trabalho)**

**MANHÃƒ (3h) - Projeto 3 FinalizaÃ§Ã£o + Artigo**:
```
â° 08:00-09:00 | Projeto 3 FinalizaÃ§Ã£o
  [ ] README.md completo
  [ ] Git commit + push
  [ ] POST #5 LinkedIn (Projeto 3 + Portfolio completo)
  
â° 09:00-11:00 | Artigo TÃ©cnico (Medium)
  [ ] Escrever artigo 1200-1500 palavras
    TÃ­tulo: "Como Construir um Data Catalog Conversacional com RAG"
  [ ] Adicionar diagramas/screenshots
  [ ] RevisÃ£o (Claude)
  [ ] Publicar no Medium
  [ ] Compartilhar no LinkedIn
```

**TARDE (3h) - Portfolio Website + Freela Platforms**:
```
â° 13:00-14:00 | Portfolio Website (Opcional mas Recomendado)
  [ ] GitHub Pages simple site
  [ ] Lista dos 3 projetos com links
  [ ] About me
  [ ] Links para LinkedIn, GitHub, Medium
  
â° 14:00-16:00 | Freela Platforms Setup
  [ ] Upwork profile completo
  [ ] Toptal application
  [ ] Fiverr Pro gigs (3-5 gigs)
  [ ] 5 aplicaÃ§Ãµes finais (total 25 na semana)
```

**NOITE (2h) - RevisÃ£o e Planejamento**:
```
â° 19:00-20:00 | RevisÃ£o Completa
  [ ] Verificar todos os 3 projetos
  [ ] Testar todos os READMEs
  [ ] Revisar perfil LinkedIn
  [ ] Lista de aplicaÃ§Ãµes enviadas
  
â° 20:00-21:00 | Planejamento Semana 2
  [ ] Priorizar follow-ups
  [ ] Agendar conversas com headhunters
  [ ] Planejar prÃ³ximos estudos (MLOps)
  [ ] Descansar! ðŸŽ‰
```

### **MÃ©tricas de Sucesso - ApÃ³s 4 Dias**

```
âœ… Portfolio TÃ©cnico:
   - 3 projetos GitHub completos e funcionais
   - 3 READMEs profissionais com badges
   - 3 demo videos (2-3 min cada)
   - Portfolio website (GitHub Pages)

âœ… PresenÃ§a Online:
   - 5+ posts LinkedIn
   - 1 artigo tÃ©cnico Medium (1200-1500 palavras)
   - Perfil LinkedIn otimizado + #OpenToWork
   - GitHub profile atualizado

âœ… Networking:
   - 25+ aplicaÃ§Ãµes de vagas
   - 15+ headhunters contatados
   - 5+ CTOs/Heads contactados
   - PresenÃ§a em 2+ comunidades tÃ©cnicas

âœ… CertificaÃ§Ã£o:
   - Microsoft AI-900

âœ… Conhecimento:
   - Prompt Engineering (DeepLearning.AI)
   - LangChain basics
   - RAG architecture
   - Vector databases

ðŸ“ˆ Expectativa ApÃ³s 1 Semana:
   - 3+ conversas agendadas com headhunters
   - 1-2 entrevistas tÃ©cnicas agendadas
   - 100+ visualizaÃ§Ãµes GitHub
   - 500+ impressÃµes LinkedIn posts

ðŸ“ˆ Expectativa ApÃ³s 2 Semanas:
   - 1+ oferta de emprego OU
   - 2+ projetos freela em negociaÃ§Ã£o OU
   - 3+ processos seletivos em andamento
   - Portfolio gerando inbound
```

---

## ðŸŽ¯ PROJETOS A IMPLEMENTAR {#projetos}

### **PROJETO 1: Database Documentation Assistant**

**Objetivo**: Gerar documentaÃ§Ã£o automÃ¡tica de databases usando LLMs para reverse engineering e anÃ¡lise contextual.

**Problema que Resolve**:
- Databases sem documentaÃ§Ã£o (comum em legados)
- ManutenÃ§Ã£o cara de documentaÃ§Ã£o manual
- Onboarding lento de novos devs
- Falta de contexto de negÃ³cio nos schemas

**Stack TecnolÃ³gico**:
```python
# Core
Python 3.10+
LangChain 0.1.0+
OpenAI API (GPT-4 para anÃ¡lise, GPT-3.5-turbo para docs)

# Database
SQLAlchemy 2.0+ (suporte MySQL, PostgreSQL, SQL Server)
psycopg2 / pymysql / pyodbc

# UI
Streamlit 1.28+

# Utilities
python-dotenv
pydantic
```

**Funcionalidades**:
1. **ConexÃ£o com Databases**: MySQL, PostgreSQL, SQL Server via SQLAlchemy
2. **ExtraÃ§Ã£o de Schema**: Tabelas, colunas, tipos, constraints, indexes
3. **AnÃ¡lise com LLM**:
   - Gerar descriÃ§Ãµes de tabelas (propÃ³sito, entidade de negÃ³cio)
   - Gerar descriÃ§Ãµes de colunas (significado, exemplos)
   - Identificar relacionamentos implÃ­citos (nÃ£o FK)
   - Sugerir data quality checks
   - Identificar potencial PII (dados sensÃ­veis LGPD)
4. **Export Markdown**: Formato dbt-compatible, README.md por schema
5. **UI Interativa**: Streamlit para configuraÃ§Ã£o e visualizaÃ§Ã£o

**Estrutura de Arquivos**:
```
01-database-documentation-assistant/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database_connector.py      # SQLAlchemy connections
â”‚   â”œâ”€â”€ schema_extractor.py        # Extract metadata
â”‚   â”œâ”€â”€ llm_documenter.py          # LLM analysis
â”‚   â”œâ”€â”€ markdown_generator.py      # Generate docs
â”‚   â””â”€â”€ config.py                  # Configuration
â”œâ”€â”€ app.py                          # Streamlit UI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ README.md
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ sample_database.sql        # Sample schema
â”‚   â””â”€â”€ output_documentation.md    # Sample output
â””â”€â”€ tests/
    â””â”€â”€ test_schema_extraction.py
```

**ImplementaÃ§Ã£o - database_connector.py**:
```python
"""Database connection handling using SQLAlchemy."""
from typing import Optional, Dict, Any
from sqlalchemy import create_engine, inspect, MetaData
from sqlalchemy.engine import Engine
import logging

logger = logging.getLogger(__name__)

class DatabaseConnector:
    """Handles database connections and metadata extraction."""
    
    def __init__(self, connection_string: str):
        """
        Initialize database connector.
        
        Args:
            connection_string: SQLAlchemy connection string
            
        Example:
            postgresql://user:pass@localhost:5432/dbname
            mysql://user:pass@localhost:3306/dbname
        """
        self.connection_string = connection_string
        self.engine: Optional[Engine] = None
        self.metadata: Optional[MetaData] = None
        
    def connect(self) -> bool:
        """
        Establish database connection.
        
        Returns:
            True if successful, False otherwise
        """
        try:
            self.engine = create_engine(self.connection_string)
            # Test connection
            with self.engine.connect() as conn:
                conn.execute("SELECT 1")
            logger.info("Database connection successful")
            return True
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            return False
    
    def get_inspector(self):
        """Get SQLAlchemy inspector for metadata extraction."""
        if not self.engine:
            raise ValueError("Not connected to database")
        return inspect(self.engine)
    
    def disconnect(self):
        """Close database connection."""
        if self.engine:
            self.engine.dispose()
            logger.info("Database connection closed")
```

**ImplementaÃ§Ã£o - schema_extractor.py**:
```python
"""Extract database schema metadata."""
from typing import List, Dict, Any
from sqlalchemy import inspect
import logging

logger = logging.getLogger(__name__)

class SchemaExtractor:
    """Extracts and structures database schema metadata."""
    
    def __init__(self, inspector):
        """
        Initialize schema extractor.
        
        Args:
            inspector: SQLAlchemy Inspector object
        """
        self.inspector = inspector
    
    def extract_all_schemas(self) -> Dict[str, Any]:
        """
        Extract complete database schema metadata.
        
        Returns:
            Dictionary with schemas, tables, columns, relationships
        """
        schemas = {}
        
        for schema_name in self.inspector.get_schema_names():
            if schema_name in ['information_schema', 'pg_catalog']:
                continue  # Skip system schemas
                
            schema_data = {
                'tables': [],
                'relationships': []
            }
            
            for table_name in self.inspector.get_table_names(schema=schema_name):
                table_data = self.extract_table_metadata(table_name, schema_name)
                schema_data['tables'].append(table_data)
            
            schemas[schema_name] = schema_data
            
        return schemas
    
    def extract_table_metadata(self, table_name: str, schema: str) -> Dict[str, Any]:
        """
        Extract metadata for a single table.
        
        Args:
            table_name: Name of the table
            schema: Schema name
            
        Returns:
            Dictionary with table metadata
        """
        columns = self.inspector.get_columns(table_name, schema=schema)
        pk = self.inspector.get_pk_constraint(table_name, schema=schema)
        fks = self.inspector.get_foreign_keys(table_name, schema=schema)
        indexes = self.inspector.get_indexes(table_name, schema=schema)
        
        # Estimate row count (if possible)
        try:
            # This is database-specific, may not work for all DBs
            row_count = self._estimate_row_count(table_name, schema)
        except:
            row_count = None
        
        return {
            'name': table_name,
            'schema': schema,
            'columns': columns,
            'primary_key': pk,
            'foreign_keys': fks,
            'indexes': indexes,
            'estimated_rows': row_count
        }
    
    def _estimate_row_count(self, table_name: str, schema: str) -> int:
        """Estimate number of rows in table (database-specific)."""
        # Implementation depends on database type
        # For PostgreSQL: SELECT reltuples FROM pg_class WHERE relname = table_name
        # For MySQL: SELECT TABLE_ROWS FROM information_schema.tables
        pass
```

**ImplementaÃ§Ã£o - llm_documenter.py**:
```python
"""LLM-based documentation generation."""
from typing import Dict, Any, List
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
import logging

logger = logging.getLogger(__name__)

class TableDocumentation(BaseModel):
    """Structured documentation for a database table."""
    business_purpose: str = Field(description="Business purpose of this table")
    entity_description: str = Field(description="What business entity this represents")
    common_use_cases: List[str] = Field(description="Common ways this table is used")
    potential_pii: List[str] = Field(description="Columns that may contain PII/sensitive data")
    data_quality_suggestions: List[str] = Field(description="Suggested data quality checks")

class ColumnDocumentation(BaseModel):
    """Structured documentation for a database column."""
    column_name: str
    business_meaning: str = Field(description="What this column represents in business terms")
    example_values: str = Field(description="Examples of typical values")
    is_sensitive: bool = Field(description="Whether this contains PII/sensitive data")

class LLMDocumenter:
    """Generates documentation using LLM analysis."""
    
    def __init__(self, model_name: str = "gpt-4", temperature: float = 0.3):
        """
        Initialize LLM documenter.
        
        Args:
            model_name: OpenAI model to use (gpt-4 for analysis, gpt-3.5-turbo for docs)
            temperature: Model temperature (lower = more focused)
        """
        self.llm = ChatOpenAI(model_name=model_name, temperature=temperature)
    
    def document_table(self, table_metadata: Dict[str, Any]) -> TableDocumentation:
        """
        Generate documentation for a table using LLM.
        
        Args:
            table_metadata: Dictionary with table schema information
            
        Returns:
            Structured table documentation
        """
        parser = PydanticOutputParser(pydantic_object=TableDocumentation)
        
        prompt = ChatPromptTemplate.from_template(
            """You are a database documentation expert. Analyze this database table and provide comprehensive documentation.

Table Name: {table_name}
Schema: {schema}
Columns: {columns}
Primary Key: {primary_key}
Foreign Keys: {foreign_keys}

Based on the table structure, infer:
1. The business purpose of this table
2. What business entity it represents
3. Common use cases
4. Which columns might contain PII or sensitive data (for LGPD compliance)
5. Suggested data quality checks

{format_instructions}
"""
        )
        
        formatted_columns = "\n".join([
            f"- {col['name']} ({col['type']}) {'NOT NULL' if not col.get('nullable', True) else ''}"
            for col in table_metadata['columns']
        ])
        
        messages = prompt.format_messages(
            table_name=table_metadata['name'],
            schema=table_metadata['schema'],
            columns=formatted_columns,
            primary_key=table_metadata['primary_key'],
            foreign_keys=table_metadata['foreign_keys'],
            format_instructions=parser.get_format_instructions()
        )
        
        response = self.llm(messages)
        return parser.parse(response.content)
    
    def document_columns(self, table_metadata: Dict[str, Any]) -> List[ColumnDocumentation]:
        """
        Generate documentation for each column in a table.
        
        Args:
            table_metadata: Dictionary with table schema information
            
        Returns:
            List of column documentation
        """
        # Similar implementation using LLM to analyze each column
        # Returns structured documentation with business meaning
        pass
    
    def identify_implicit_relationships(self, schemas: Dict[str, Any]) -> List[Dict[str, str]]:
        """
        Use LLM to identify implicit relationships not defined by foreign keys.
        
        Args:
            schemas: Complete schema metadata
            
        Returns:
            List of potential relationships (table_a, column_a, table_b, column_b, confidence)
        """
        # LLM analyzes column names and data types to find naming patterns
        # Example: "customer_id" in orders table -> "id" in customers table
        pass
```

**README.md Template**:
```markdown
# ðŸ—„ï¸ Database Documentation Assistant

![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![LangChain](https://img.shields.io/badge/LangChain-0.1.0+-green.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

Generate comprehensive, AI-powered documentation for databases automatically. Ideal for legacy systems, onboarding, and maintaining up-to-date data catalogs.

## ðŸŽ¯ Problem It Solves

- **Legacy databases without documentation** - Common in large organizations
- **Expensive manual documentation** - Hundreds of hours of DBA/Data Engineer time
- **Outdated documentation** - Documentation gets stale as schema evolves
- **Missing business context** - Technical schemas lack business meaning

## âœ¨ Features

- ðŸ”Œ **Multi-Database Support**: MySQL, PostgreSQL, SQL Server
- ðŸ¤– **LLM-Powered Analysis**: Uses GPT-4 to understand business context
- ðŸ“ **Comprehensive Documentation**: Table purposes, column meanings, relationships
- ðŸ”’ **PII Detection**: Identifies potential sensitive data for LGPD/GDPR compliance
- âœ… **Quality Suggestions**: AI-generated data quality check recommendations
- ðŸ“Š **dbt-Compatible Output**: Export as Markdown for dbt docs
- ðŸŽ¨ **Interactive UI**: Streamlit interface for easy configuration

## ðŸš€ Quick Start

### Installation

\`\`\`bash
# Clone repository
git clone https://github.com/paulocesarjunior/database-documentation-assistant.git
cd database-documentation-assistant

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY
\`\`\`

### Usage

\`\`\`bash
# Run Streamlit app
streamlit run app.py
\`\`\`

Then:
1. Enter database connection string
2. Select schemas/tables to document
3. Click "Generate Documentation"
4. Download Markdown files

### Example Output

See [examples/output_documentation.md](examples/output_documentation.md) for sample generated documentation.

## ðŸ“– How It Works

1. **Connect**: Uses SQLAlchemy to connect to database
2. **Extract**: Retrieves schema metadata (tables, columns, constraints)
3. **Analyze**: LLM analyzes structure to infer business meaning
4. **Generate**: Creates comprehensive Markdown documentation
5. **Export**: Outputs dbt-compatible documentation

## ðŸ—ï¸ Architecture

\`\`\`
User Input (Connection String)
         â†“
   database_connector.py (SQLAlchemy)
         â†“
   schema_extractor.py (Metadata)
         â†“
   llm_documenter.py (GPT-4 Analysis)
         â†“
   markdown_generator.py (Output)
         â†“
   Markdown Documentation Files
\`\`\`

## ðŸ› ï¸ Tech Stack

- **Python 3.10+**: Core language
- **LangChain**: LLM orchestration
- **OpenAI GPT-4**: Schema analysis
- **SQLAlchemy**: Database connectivity
- **Streamlit**: Web UI
- **Pydantic**: Data validation

## ðŸ“Š Results

- **2 weeks â†’ 2 hours**: Documentation time reduction
- **100% coverage**: All tables documented
- **Business context**: AI-inferred purpose and use cases
- **LGPD compliant**: PII identification

## ðŸ”’ Security

- Credentials stored in `.env` (not committed)
- No data extraction, only metadata
- Read-only database access recommended

## ðŸ“ License

MIT License - see [LICENSE](LICENSE)

## ðŸ‘¤ Author

**Paulo Cesar Mendes de Sousa Junior**
- LinkedIn: [linkedin.com/in/paulocesarjr](https://linkedin.com/in/paulocesarjr)
- GitHub: [@paulocesarjunior](https://github.com/paulocesarjunior)
- Portfolio: [paulocesarjr.dev](https://paulocesarjr.dev)

## ðŸ™ Acknowledgments

Built as part of career transition to Data + AI Engineering.
```

**Tempo Estimado (Com Claude)**: 3-4 horas

---

### **PROJETO 2: Data Lineage Analyzer with AI**

**Objetivo**: Mapear automaticamente data lineage (origem â†’ transformaÃ§Ã£o â†’ destino) a partir de SQL e dbt, usando LLMs para explicar lÃ³gica de negÃ³cio.

**Problema que Resolve**:
- Lineage manual Ã© tedioso e propenso a erros
- DifÃ­cil entender impacto de mudanÃ§as em pipelines
- Sistemas complexos com centenas de transformaÃ§Ãµes
- Falta de documentaÃ§Ã£o de lÃ³gica de negÃ³cio

**Stack TecnolÃ³gico**:
```python
# Core
Python 3.10+
sqlparse / sqlglot (SQL parsing)
LangChain
OpenAI API

# Graph & Visualization
NetworkX (graph algorithms)
Plotly (interactive graphs)

# UI
Streamlit

# dbt Integration
pyyaml (parse dbt manifest.json)
```

**Funcionalidades**:
1. **Parse SQL**: Complexo (CTEs, subqueries, JOINs, UNIONs)
2. **Parse dbt**: manifest.json, models, sources, tests
3. **Extract Dependencies**: Tabelas fonte â†’ transformaÃ§Ãµes â†’ destino
4. **LLM Explanation**: Explicar em linguagem natural o que transformaÃ§Ã£o faz
5. **Graph Visualization**: NetworkX + Plotly interativo, zoom, filtros
6. **Impact Analysis**: "Se mudar tabela X, quais downstream sÃ£o afetados?"
7. **Export**: OpenLineage format, JSON, GraphML

**Estrutura de Arquivos**:
```
02-data-lineage-analyzer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sql_parser.py              # sqlglot-based parser
â”‚   â”œâ”€â”€ dbt_parser.py              # dbt manifest.json parser
â”‚   â”œâ”€â”€ dependency_extractor.py    # Extract table dependencies
â”‚   â”œâ”€â”€ llm_analyzer.py            # LLM explains transformations
â”‚   â”œâ”€â”€ graph_builder.py           # NetworkX graph
â”‚   â”œâ”€â”€ visualizer.py              # Plotly interactive viz
â”‚   â”œâ”€â”€ impact_analyzer.py         # Impact analysis
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ app.py                          # Streamlit UI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ README.md
â””â”€â”€ examples/
    â”œâ”€â”€ sample_sql_files/
    â”œâ”€â”€ sample_dbt_project/
    â””â”€â”€ sample_lineage_output.json
```

**ImplementaÃ§Ã£o - sql_parser.py**:
```python
"""SQL parsing using sqlglot for robust dependency extraction."""
import sqlglot
from sqlglot import parse_one, exp
from typing import List, Dict, Set, Tuple
import logging

logger = logging.getLogger(__name__)

class SQLParser:
    """Parses SQL to extract table dependencies."""
    
    def __init__(self, dialect: str = "postgres"):
        """
        Initialize SQL parser.
        
        Args:
            dialect: SQL dialect (postgres, mysql, tsql, etc.)
        """
        self.dialect = dialect
    
    def parse_sql_file(self, sql_content: str) -> Dict[str, Any]:
        """
        Parse SQL content and extract dependencies.
        
        Args:
            sql_content: SQL query as string
            
        Returns:
            Dictionary with source tables, target table, joins, filters
        """
        try:
            # Parse SQL into AST
            ast = parse_one(sql_content, dialect=self.dialect)
            
            # Extract components
            source_tables = self._extract_source_tables(ast)
            target_table = self._extract_target_table(ast)
            joins = self._extract_joins(ast)
            filters = self._extract_filters(ast)
            aggregations = self._extract_aggregations(ast)
            
            return {
                'source_tables': source_tables,
                'target_table': target_table,
                'joins': joins,
                'filters': filters,
                'aggregations': aggregations,
                'raw_sql': sql_content
            }
            
        except Exception as e:
            logger.error(f"SQL parsing failed: {e}")
            return None
    
    def _extract_source_tables(self, ast) -> List[Dict[str, str]]:
        """Extract all source tables from SQL AST."""
        tables = []
        
        for table in ast.find_all(exp.Table):
            tables.append({
                'schema': table.db if table.db else None,
                'table': table.name,
                'alias': table.alias if table.alias else None
            })
        
        return tables
    
    def _extract_target_table(self, ast) -> Dict[str, str]:
        """Extract target table (for INSERT INTO, CREATE TABLE AS, etc.)."""
        # Check if statement is INSERT, CREATE TABLE, or CREATE VIEW
        if isinstance(ast, (exp.Insert, exp.Create)):
            table = ast.find(exp.Table)
            if table:
                return {
                    'schema': table.db if table.db else None,
                    'table': table.name
                }
        return None
    
    def _extract_joins(self, ast) -> List[Dict[str, Any]]:
        """Extract JOIN conditions."""
        joins = []
        
        for join in ast.find_all(exp.Join):
            joins.append({
                'type': join.side,  # LEFT, RIGHT, INNER, etc.
                'table': str(join.this),
                'condition': str(join.args.get('on', ''))
            })
        
        return joins
    
    def _extract_filters(self, ast) -> List[str]:
        """Extract WHERE clause filters."""
        filters = []
        
        for where in ast.find_all(exp.Where):
            filters.append(str(where.this))
        
        return filters
    
    def _extract_aggregations(self, ast) -> List[str]:
        """Extract GROUP BY and aggregation functions."""
        aggregations = []
        
        # GROUP BY
        for group_by in ast.find_all(exp.Group):
            aggregations.append(f"GROUP BY {str(group_by)}")
        
        # Aggregation functions (SUM, COUNT, AVG, etc.)
        for agg in ast.find_all(exp.AggFunc):
            aggregations.append(f"{agg.sql_name()}({str(agg.this)})")
        
        return aggregations
```

**ImplementaÃ§Ã£o - llm_analyzer.py**:
```python
"""LLM-based transformation explanation."""
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

class LLMAnalyzer:
    """Uses LLM to explain data transformations in natural language."""
    
    def __init__(self, model_name: str = "gpt-4"):
        """Initialize LLM analyzer."""
        self.llm = ChatOpenAI(model_name=model_name, temperature=0.3)
    
    def explain_transformation(self, sql_metadata: Dict[str, Any]) -> str:
        """
        Generate natural language explanation of SQL transformation.
        
        Args:
            sql_metadata: Parsed SQL metadata (sources, joins, filters, etc.)
            
        Returns:
            Human-readable explanation
        """
        prompt = ChatPromptTemplate.from_template(
            """You are a data engineer explaining a SQL transformation to a business analyst.

**Source Tables**: {source_tables}
**Target Table**: {target_table}
**Joins**: {joins}
**Filters**: {filters}
**Aggregations**: {aggregations}

**SQL Query**:
```sql
{sql_query}
```

Explain in 2-3 sentences what this transformation does from a business perspective. Focus on:
1. What data is being combined
2. What business logic is applied
3. What the final output represents

Use simple, non-technical language.
"""
        )
        
        messages = prompt.format_messages(
            source_tables=sql_metadata['source_tables'],
            target_table=sql_metadata['target_table'],
            joins=sql_metadata['joins'],
            filters=sql_metadata['filters'],
            aggregations=sql_metadata['aggregations'],
            sql_query=sql_metadata['raw_sql']
        )
        
        response = self.llm(messages)
        return response.content
    
    def explain_impact(self, table_name: str, downstream_tables: List[str]) -> str:
        """
        Explain impact of changing a table on downstream dependencies.
        
        Args:
            table_name: Table being changed
            downstream_tables: List of tables that depend on it
            
        Returns:
            Impact explanation
        """
        prompt = ChatPromptTemplate.from_template(
            """A data engineer wants to modify table '{table_name}'.

**Downstream Dependencies** (tables that read from {table_name}):
{downstream_list}

Explain the potential impact of changing {table_name} on these downstream tables. Consider:
1. What could break
2. What validation is needed
3. What stakeholders should be notified

Keep it concise (3-4 sentences).
"""
        )
        
        downstream_list = "\n".join([f"- {table}" for table in downstream_tables])
        
        messages = prompt.format_messages(
            table_name=table_name,
            downstream_list=downstream_list
        )
        
        response = self.llm(messages)
        return response.content
```

**ImplementaÃ§Ã£o - graph_builder.py**:
```python
"""Build NetworkX graph from lineage data."""
import networkx as nx
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)

class GraphBuilder:
    """Builds directed graph of data lineage."""
    
    def __init__(self):
        """Initialize graph builder."""
        self.graph = nx.DiGraph()
    
    def add_transformation(self, sql_metadata: Dict[str, Any], explanation: str):
        """
        Add a transformation to the lineage graph.
        
        Args:
            sql_metadata: Parsed SQL metadata
            explanation: LLM-generated explanation
        """
        target = sql_metadata['target_table']
        
        if target:
            # Add target node
            target_id = f"{target['schema']}.{target['table']}" if target['schema'] else target['table']
            self.graph.add_node(target_id, type='table', explanation=explanation)
            
            # Add source nodes and edges
            for source in sql_metadata['source_tables']:
                source_id = f"{source['schema']}.{source['table']}" if source['schema'] else source['table']
                self.graph.add_node(source_id, type='table')
                
                # Add edge with transformation metadata
                self.graph.add_edge(
                    source_id,
                    target_id,
                    transformation=sql_metadata['raw_sql'][:200],  # Truncate for display
                    joins=sql_metadata['joins'],
                    filters=sql_metadata['filters']
                )
    
    def get_upstream_tables(self, table_name: str, depth: int = None) -> List[str]:
        """
        Get all upstream dependencies of a table.
        
        Args:
            table_name: Table to analyze
            depth: Maximum depth to traverse (None = unlimited)
            
        Returns:
            List of upstream table names
        """
        if table_name not in self.graph:
            return []
        
        if depth is None:
            # Get all ancestors
            return list(nx.ancestors(self.graph, table_name))
        else:
            # BFS with depth limit
            upstream = set()
            queue = [(table_name, 0)]
            visited = set()
            
            while queue:
                node, current_depth = queue.pop(0)
                
                if node in visited or current_depth >= depth:
                    continue
                
                visited.add(node)
                
                for predecessor in self.graph.predecessors(node):
                    upstream.add(predecessor)
                    queue.append((predecessor, current_depth + 1))
            
            return list(upstream)
    
    def get_downstream_tables(self, table_name: str, depth: int = None) -> List[str]:
        """Get all downstream dependencies of a table."""
        if table_name not in self.graph:
            return []
        
        if depth is None:
            return list(nx.descendants(self.graph, table_name))
        else:
            # Similar to upstream but using successors
            downstream = set()
            queue = [(table_name, 0)]
            visited = set()
            
            while queue:
                node, current_depth = queue.pop(0)
                
                if node in visited or current_depth >= depth:
                    continue
                
                visited.add(node)
                
                for successor in self.graph.successors(node):
                    downstream.add(successor)
                    queue.append((successor, current_depth + 1))
            
            return list(downstream)
    
    def calculate_impact_score(self, table_name: str) -> float:
        """
        Calculate impact score (0-100) based on downstream dependencies.
        
        Higher score = more critical table (many downstream dependencies)
        """
        if table_name not in self.graph:
            return 0.0
        
        downstream_count = len(self.get_downstream_tables(table_name))
        
        # Normalize to 0-100 scale (max 50 downstream = 100)
        return min(100.0, (downstream_count / 50.0) * 100)
    
    def export_to_dict(self) -> Dict[str, Any]:
        """Export graph as dictionary for JSON serialization."""
        return {
            'nodes': [
                {
                    'id': node,
                    **self.graph.nodes[node]
                }
                for node in self.graph.nodes()
            ],
            'edges': [
                {
                    'source': u,
                    'target': v,
                    **self.graph[u][v]
                }
                for u, v in self.graph.edges()
            ]
        }
```

**README.md** (similar structure, focusing on lineage use cases)

**Tempo Estimado (Com Claude)**: 4-5 horas

---

### **PROJETO 3: RAG sobre DicionÃ¡rio de Dados**

**Objetivo**: Chat conversacional para consultar estrutura de dados corporativos usando RAG (Retrieval-Augmented Generation).

**Problema que Resolve**:
- Analistas gastam horas procurando "onde estÃ¡ campo X?"
- DicionÃ¡rios de dados sÃ£o longos e difÃ­ceis de navegar
- Onboarding lento (novos membros nÃ£o sabem onde dados estÃ£o)
- Perguntas repetitivas para engenheiros de dados

**Stack TecnolÃ³gico**:
```python
# Core
Python 3.10+
LangChain
OpenAI API (embeddings + chat)

# Vector Database
ChromaDB (local, persistent)
# ou Pinecone (cloud, production)

# Document Processing
pypdf2 (PDFs)
beautifulsoup4 (HTML)
markdown (Markdown)

# UI
Streamlit
```

**Funcionalidades**:
1. **Ingest Documents**: SQL schemas, dbt docs, READMEs, PDFs de documentaÃ§Ã£o
2. **Generate Embeddings**: OpenAI text-embedding-ada-002
3. **Semantic Search**: Encontrar trechos relevantes via similarity
4. **RAG Chain**: LLM responde perguntas usando contexto recuperado
5. **Cite Sources**: Sempre citar de onde veio informaÃ§Ã£o
6. **Conversational**: MemÃ³ria de conversa, follow-up questions
7. **Filters**: Filtrar por schema, domÃ­nio, tipo de documento

**Sample Questions**:
```
"Onde estÃ¡ armazenado o CPF do cliente?"
â†’ RAG retrieves: customers table (column cpf_cliente), orders table (via FK)

"Quais tabelas contÃªm dados de vendas?"
â†’ RAG retrieves: sales, order_items, invoices

"Como Ã© calculada a receita lÃ­quida?"
â†’ RAG retrieves: dbt model revenue_net.sql with formula

"Qual a diferenÃ§a entre customer_id e client_id?"
â†’ RAG retrieves: glossary entries explaining terms

"Preciso dados de NPS dos Ãºltimos 6 meses, onde busco?"
â†’ RAG retrieves: nps_scores table, customer_feedback table
```

**Estrutura de Arquivos**:
```
03-rag-data-dictionary/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_loader.py         # Load SQL, dbt, PDFs, etc.
â”‚   â”œâ”€â”€ chunking.py                # Intelligent chunking
â”‚   â”œâ”€â”€ embedding_generator.py     # OpenAI embeddings
â”‚   â”œâ”€â”€ vector_store.py            # ChromaDB/Pinecone
â”‚   â”œâ”€â”€ rag_chain.py               # LangChain RAG chain
â”‚   â”œâ”€â”€ chat_interface.py          # Conversational interface
â”‚   â”œâ”€â”€ feedback_logger.py         # Log user feedback
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ app.py                          # Streamlit chat UI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ README.md
â”œâ”€â”€ data/                           # Sample data
â”‚   â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ dbt_docs/
â”‚   â””â”€â”€ pdfs/
â””â”€â”€ vector_db/                      # Persistent vector database
```

**ImplementaÃ§Ã£o - document_loader.py**:
```python
"""Load and parse various document formats."""
from typing import List, Dict, Any
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class DocumentLoader:
    """Loads documents from various sources and formats."""
    
    def __init__(self):
        """Initialize document loader."""
        pass
    
    def load_directory(self, directory_path: str) -> List[Dict[str, Any]]:
        """
        Load all documents from a directory recursively.
        
        Args:
            directory_path: Path to directory
            
        Returns:
            List of documents with content and metadata
        """
        documents = []
        path = Path(directory_path)
        
        # Supported file types
        loaders = {
            '.sql': self._load_sql,
            '.md': self._load_markdown,
            '.txt': self._load_text,
            '.pdf': self._load_pdf,
            '.yml': self._load_yaml,
            '.yaml': self._load_yaml
        }
        
        for file_path in path.rglob('*'):
            if file_path.is_file() and file_path.suffix in loaders:
                try:
                    content = loaders[file_path.suffix](file_path)
                    documents.append({
                        'content': content,
                        'metadata': {
                            'source': str(file_path),
                            'file_type': file_path.suffix,
                            'file_name': file_path.name
                        }
                    })
                except Exception as e:
                    logger.error(f"Failed to load {file_path}: {e}")
        
        logger.info(f"Loaded {len(documents)} documents from {directory_path}")
        return documents
    
    def _load_sql(self, file_path: Path) -> str:
        """Load SQL file."""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def _load_markdown(self, file_path: Path) -> str:
        """Load Markdown file."""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def _load_text(self, file_path: Path) -> str:
        """Load text file."""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def _load_pdf(self, file_path: Path) -> str:
        """Load PDF file."""
        # Usar pypdf2 ou pdfplumber
        pass
    
    def _load_yaml(self, file_path: Path) -> str:
        """Load YAML file (dbt models, etc.)."""
        import yaml
        with open(file_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        # Convert to readable format
        return yaml.dump(data, default_flow_style=False)
```

**ImplementaÃ§Ã£o - rag_chain.py**:
```python
"""RAG chain for question answering over data dictionary."""
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import ChatPromptTemplate
from langchain.memory import ConversationBufferMemory
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

class RAGChain:
    """Retrieval-Augmented Generation chain for data dictionary Q&A."""
    
    def __init__(self, vector_store, model_name: str = "gpt-4"):
        """
        Initialize RAG chain.
        
        Args:
            vector_store: Vector store (ChromaDB/Pinecone) instance
            model_name: OpenAI model name
        """
        self.vector_store = vector_store
        self.llm = ChatOpenAI(model_name=model_name, temperature=0.2)
        
        # Conversational memory
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
        # Custom prompt
        self.prompt = ChatPromptTemplate.from_template(
            """You are a helpful data catalog assistant. Answer questions about the organization's data structure using the provided context.

**Context from Data Dictionary**:
{context}

**Chat History**:
{chat_history}

**User Question**: {question}

**Instructions**:
1. Answer based ONLY on the provided context
2. If the context doesn't contain the answer, say "I don't have that information in the data dictionary"
3. Always cite the source (file name) where you found the information
4. Be concise but complete
5. Use technical terms appropriately for data professionals

**Answer**:
"""
        )
        
        # Create retrieval chain
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(search_kwargs={"k": 5}),
            return_source_documents=True,
            chain_type_kwargs={"prompt": self.prompt}
        )
    
    def ask(self, question: str) -> Dict[str, Any]:
        """
        Ask a question about the data dictionary.
        
        Args:
            question: User's question
            
        Returns:
            Dictionary with answer and sources
        """
        # Get chat history
        chat_history = self.memory.load_memory_variables({})
        
        # Run query
        result = self.qa_chain({
            "query": question,
            "chat_history": chat_history.get("chat_history", [])
        })
        
        # Extract sources
        sources = []
        if result.get("source_documents"):
            for doc in result["source_documents"]:
                sources.append({
                    'file': doc.metadata.get('source', 'Unknown'),
                    'content_snippet': doc.page_content[:200]  # First 200 chars
                })
        
        # Save to memory
        self.memory.save_context(
            {"input": question},
            {"output": result["result"]}
        )
        
        return {
            'answer': result["result"],
            'sources': sources,
            'question': question
        }
    
    def reset_conversation(self):
        """Clear conversation history."""
        self.memory.clear()
```

**Streamlit UI (app.py)**:
```python
"""Streamlit chat interface for RAG Data Dictionary."""
import streamlit as st
from src.rag_chain import RAGChain
from src.vector_store import VectorStore

st.set_page_config(page_title="Data Dictionary Chat", page_icon="ðŸ“š", layout="wide")

st.title("ðŸ“š Data Dictionary Chat")
st.markdown("Ask questions about your data structure in natural language!")

# Initialize components (cache for performance)
@st.cache_resource
def load_rag_chain():
    vector_store = VectorStore()
    vector_store.load_from_disk("./vector_db")
    return RAGChain(vector_store)

rag_chain = load_rag_chain()

# Chat interface
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message.get("sources"):
            with st.expander("ðŸ“Ž Sources"):
                for source in message["sources"]:
                    st.text(f"â€¢ {source['file']}")

# User input
if prompt := st.chat_input("Ask about your data..."):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Get response
    with st.chat_message("assistant"):
        with st.spinner("Searching data dictionary..."):
            response = rag_chain.ask(prompt)
        
        st.markdown(response["answer"])
        
        if response["sources"]:
            with st.expander("ðŸ“Ž Sources"):
                for source in response["sources"]:
                    st.text(f"â€¢ {source['file']}")
                    st.caption(source['content_snippet'])
    
    # Add assistant message
    st.session_state.messages.append({
        "role": "assistant",
        "content": response["answer"],
        "sources": response["sources"]
    })

# Sidebar
with st.sidebar:
    st.markdown("### ðŸ’¡ Sample Questions")
    st.markdown("""
    - Where is customer CPF stored?
    - Which tables contain sales data?
    - How is net revenue calculated?
    - What's the difference between customer_id and client_id?
    - I need NPS data from last 6 months, where do I find it?
    """)
    
    if st.button("ðŸ—‘ï¸ Clear Chat"):
        st.session_state.messages = []
        rag_chain.reset_conversation()
        st.rerun()
```

**Tempo Estimado (Com Claude)**: 5-6 horas

---

## ðŸ“ PADRÃ•ES E CONVENÃ‡Ã•ES {#padroes}

### **PadrÃµes de CÃ³digo Python**

**Estilo**:
- **PEP 8 compliant**: Usar ferramentas como `black` (formatter) e `flake8` (linter)
- **Line length**: 88 characters (black default) ou 100 (mais moderno)
- **Imports**: Agrupados (stdlib, third-party, local), ordenados alfabeticamente
- **Naming**:
  - Classes: `PascalCase`
  - Functions/methods: `snake_case`
  - Constants: `UPPER_SNAKE_CASE`
  - Private: `_leading_underscore`

**Type Hints** (ObrigatÃ³rio):
```python
from typing import List, Dict, Optional, Any, Union

def process_data(
    input_data: List[Dict[str, Any]],
    filter_key: Optional[str] = None,
    limit: int = 100
) -> Dict[str, Union[int, List[str]]]:
    """
    Process input data with optional filtering.
    
    Args:
        input_data: List of dictionaries to process
        filter_key: Optional key to filter by
        limit: Maximum number of results
        
    Returns:
        Dictionary with count and processed items
        
    Raises:
        ValueError: If input_data is empty
    """
    if not input_data:
        raise ValueError("input_data cannot be empty")
    
    # Implementation
    pass
```

**Docstrings** (Google Style):
```python
def calculate_metrics(
    data: pd.DataFrame,
    groupby_columns: List[str]
) -> pd.DataFrame:
    """
    Calculate aggregated metrics from DataFrame.
    
    Computes sum, mean, and count for numerical columns
    grouped by specified columns.
    
    Args:
        data: Input DataFrame with numerical data
        groupby_columns: Columns to group by
        
    Returns:
        DataFrame with aggregated metrics
        
    Raises:
        KeyError: If groupby_columns not in DataFrame
        
    Example:
        >>> df = pd.DataFrame({'category': ['A', 'A', 'B'], 'value': [1, 2, 3]})
        >>> calculate_metrics(df, ['category'])
               value_sum  value_mean  value_count
        category                                  
        A              3         1.5            2
        B              3         3.0            1
    """
    pass
```

**Error Handling**:
```python
import logging

logger = logging.getLogger(__name__)

def risky_operation(param: str) -> Optional[str]:
    """
    Perform operation that may fail.
    
    Always use try-except with specific exceptions.
    Always log errors with context.
    """
    try:
        result = potentially_failing_function(param)
        logger.info(f"Operation successful for param={param}")
        return result
    except ValueError as e:
        logger.error(f"Invalid parameter {param}: {e}")
        raise  # Re-raise if caller should handle
    except Exception as e:
        logger.error(f"Unexpected error in risky_operation: {e}", exc_info=True)
        return None  # Or raise, depending on design
```

**Logging** (NÃ£o usar `print`!):
```python
import logging

# Configure logging (app.py or __main__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# Use logging instead of print
logger.debug("Detailed debug information")
logger.info("General information")
logger.warning("Warning message")
logger.error("Error occurred", exc_info=True)  # Include traceback
logger.critical("Critical error")
```

### **PadrÃµes SQL**

**Estilo**:
```sql
-- Lowercase keywords (mais moderno)
-- IndentaÃ§Ã£o clara
-- VÃ­rgulas leading (antes, nÃ£o depois)

-- âŒ BAD
SELECT col1, col2, col3 FROM table1 WHERE col1 = 'value' AND col2 > 10

-- âœ… GOOD
select
    col1
    , col2
    , col3
from table1
where
    col1 = 'value'
    and col2 > 10

-- CTEs para queries complexas
with customer_orders as (
    select
        customer_id
        , count(*) as order_count
        , sum(total) as total_spent
    from orders
    where
        order_date >= '2024-01-01'
    group by customer_id
)

select
    c.customer_name
    , co.order_count
    , co.total_spent
from customers c
inner join customer_orders co
    on c.id = co.customer_id
where
    co.total_spent > 1000
order by co.total_spent desc
limit 100;
```

**ComentÃ¡rios**:
```sql
-- Complex logic deserves comments
-- Explain WHY, not WHAT

-- Calculate rolling 7-day average
-- Business rule: exclude weekends
select
    date
    , avg(value) over (
        order by date
        rows between 6 preceding and current row
    ) as rolling_avg
from daily_metrics
where
    extract(dow from date) not in (0, 6)  -- 0=Sunday, 6=Saturday
```

### **Git Commit Messages**

**Formato**:
```
<type>(<scope>): <subject>

<body (optional)>

<footer (optional)>
```

**Types**:
- `feat`: Nova feature
- `fix`: Bug fix
- `docs`: DocumentaÃ§Ã£o
- `style`: Formatting (nÃ£o afeta lÃ³gica)
- `refactor`: Code refactoring
- `test`: Adicionar testes
- `chore`: ManutenÃ§Ã£o (dependencies, config)

**Examples**:
```bash
feat(database-connector): add support for SQL Server

- Implemented pyodbc connection
- Added connection string validation
- Updated tests for SQL Server

fix(llm-documenter): handle timeout errors gracefully

Previously, OpenAI API timeouts would crash the app.
Now retries 3 times with exponential backoff.

docs(readme): add installation instructions for Windows

refactor(sql-parser): extract common logic to base class

test(schema-extractor): add unit tests for foreign keys

chore(deps): update langchain to 0.1.5
```

### **Requirements.txt**

**Organiziado por categoria**:
```txt
# Core
langchain==0.1.0
openai==1.3.0
anthropic==0.8.0

# Databases
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
pymysql==1.1.0

# Vector Stores
chromadb==0.4.22
pinecone-client==3.0.0

# Data Processing
pandas==2.1.4
numpy==1.26.2

# Visualization
plotly==5.18.0
networkx==3.2.1

# Web Framework
streamlit==1.28.0

# Utilities
python-dotenv==1.0.0
pydantic==2.5.3
pyyaml==6.0.1

# Development
pytest==7.4.3
black==23.12.1
flake8==7.0.0

# SQL Parsing
sqlglot==20.9.0
sqlparse==0.4.4
```

**Pin major + minor versions** (permite patch updates):
```txt
# âœ… GOOD - allows 2.0.x updates
sqlalchemy==2.0.23

# âŒ BAD - too strict
sqlalchemy==2.0.23.0

# âŒ BAD - too loose
sqlalchemy>=2.0
```

### **README.md Template**

**Estrutura PadrÃ£o**:
```markdown
# ðŸŽ¯ Project Title

![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Status](https://img.shields.io/badge/status-active-success.svg)

One-sentence description of what the project does.

## ðŸ“– Table of Contents

- [Problem](#problem)
- [Solution](#solution)
- [Features](#features)
- [Quick Start](#quick-start)
- [Usage](#usage)
- [Architecture](#architecture)
- [Tech Stack](#tech-stack)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)
- [Author](#author)

## ðŸŽ¯ Problem

Describe the problem this project solves.

## âœ¨ Solution

How this project solves the problem.

## ðŸš€ Features

- Feature 1
- Feature 2
- Feature 3

## ðŸƒ Quick Start

### Prerequisites

\`\`\`bash
python --version  # 3.10+
\`\`\`

### Installation

\`\`\`bash
git clone https://github.com/username/project.git
cd project
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
\`\`\`

### Configuration

\`\`\`bash
cp .env.example .env
# Edit .env and add your API keys
\`\`\`

### Run

\`\`\`bash
streamlit run app.py
\`\`\`

## ðŸ’¡ Usage

### Example 1

\`\`\`python
from src import MyClass

obj = MyClass()
result = obj.do_something()
\`\`\`

### Example 2

Screenshots or GIFs showing the UI.

## ðŸ—ï¸ Architecture

\`\`\`
Component A
    â†“
Component B
    â†“
Component C
\`\`\`

## ðŸ› ï¸ Tech Stack

- **Python 3.10+**: Core language
- **LangChain**: LLM orchestration
- **OpenAI**: GPT models
- **Streamlit**: Web UI

## ðŸ“Š Results

- Metric 1: X% improvement
- Metric 2: Y hours saved

## ðŸ¤ Contributing

Contributions welcome! Please open an issue or PR.

## ðŸ“ License

MIT License - see [LICENSE](LICENSE)

## ðŸ‘¤ Author

**Your Name**
- LinkedIn: [link]
- GitHub: [@username]
- Portfolio: [link]

## ðŸ™ Acknowledgments

- Thanks to X for Y
- Inspired by Z
\`\`\`

### **.gitignore**

```gitignore
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/
ENV/
*.egg-info/
dist/
build/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# Environment
.env
.env.local

# Data
*.csv
*.xlsx
*.db
*.sqlite
data/
vector_db/

# Logs
*.log
logs/

# OS
.DS_Store
Thumbs.db

# Jupyter
.ipynb_checkpoints/

# Testing
.pytest_cache/
.coverage
htmlcov/

# Streamlit
.streamlit/
```

---

## ðŸ”„ FLUXO DE TRABALHO {#fluxo-trabalho}

### **Workflow com Claude AI (Pair Programming)**

**PrincÃ­pio**: Claude AI age como pair programmer experiente, gerando cÃ³digo completo, testÃ¡vel, documentado.

### **Ciclo de Desenvolvimento**

```
1. PLANEJAMENTO (VocÃª + Claude)
   [ ] Definir feature/mÃ³dulo a implementar
   [ ] Especificar inputs/outputs
   [ ] Decidir estrutura de arquivos
   
2. IMPLEMENTAÃ‡ÃƒO (Claude)
   [ ] VocÃª: "Implementar database_connector.py conforme especificaÃ§Ã£o"
   [ ] Claude: Gera cÃ³digo completo com:
       - Type hints
       - Docstrings
       - Error handling
       - Logging
       - ComentÃ¡rios
   
3. TESTE (VocÃª)
   [ ] Copiar cÃ³digo gerado para arquivo
   [ ] Executar testes manuais
   [ ] Identificar bugs/ajustes necessÃ¡rios
   
4. REFINAMENTO (Claude)
   [ ] VocÃª: "Fix erro X", "Add feature Y"
   [ ] Claude: Ajusta cÃ³digo
   [ ] Repetir atÃ© funcionar
   
5. DOCUMENTAÃ‡ÃƒO (Claude)
   [ ] VocÃª: "Gerar README.md para este projeto"
   [ ] Claude: README completo com badges, exemplos, etc.
   
6. COMMIT (VocÃª)
   [ ] git add .
   [ ] git commit -m "feat: add database connector"
   [ ] git push
```

### **Como Pedir CÃ³digo ao Claude**

**âŒ Ruim**:
```
"Faz um cÃ³digo que conecta no banco de dados"
```

**âœ… Bom**:
```
"Implementar database_connector.py seguindo a especificaÃ§Ã£o do PROJETO 1.

Requisitos:
- Classe DatabaseConnector
- MÃ©todo connect(connection_string: str) -> bool
- MÃ©todo get_inspector() retorna SQLAlchemy Inspector
- MÃ©todo disconnect()
- Type hints obrigatÃ³rios
- Docstrings estilo Google
- Error handling com logging
- Suportar PostgreSQL e MySQL

Seguir padrÃµes de cÃ³digo definidos neste documento."
```

**âœ… Ainda Melhor (Com Contexto)**:
```
"Implementar llm_documenter.py para PROJETO 1 - Database Documentation Assistant.

Este mÃ³dulo usa LLM para analisar schemas de banco e gerar documentaÃ§Ã£o.

Classe LLMDocumenter:
- __init__(model_name: str = "gpt-4")
- document_table(table_metadata: Dict) -> TableDocumentation
- document_columns(table_metadata: Dict) -> List[ColumnDocumentation]
- identify_implicit_relationships(schemas: Dict) -> List[Dict]

Use LangChain + Pydantic output parsers.
Inclua prompts completos para o LLM.
Seguir padrÃµes de cÃ³digo deste documento.

Exemplo de uso no README.md."
```

### **Como Revisar CÃ³digo com Claude**

```
"Revisar o cÃ³digo em database_connector.py quanto a:

1. PEP 8 compliance
2. Type hints completos
3. Error handling robusto
4. Logging adequado
5. Docstrings completas
6. Testes de edge cases

Sugerir melhorias."
```

### **Como Gerar README com Claude**

```
"Gerar README.md completo para o projeto 'Database Documentation Assistant'.

Incluir:
- Badges (Python version, license, status)
- DescriÃ§Ã£o problema/soluÃ§Ã£o
- Features
- Quick Start (instalaÃ§Ã£o, configuraÃ§Ã£o, uso)
- Exemplos de cÃ³digo
- Screenshots (placeholders)
- Arquitetura
- Tech Stack
- Results/metrics
- SeÃ§Ã£o Author com meus dados

Seguir template de README definido neste documento.
Tom profissional mas acessÃ­vel."
```

### **Como Criar Posts LinkedIn com Claude**

```
"Criar post LinkedIn anunciando publicaÃ§Ã£o do projeto 'Database Documentation Assistant'.

Tom:
- Profissional mas pessoal
- Foco no problema que resolve
- Mencionar stack tÃ©cnica
- Call-to-action para ver projeto
- Incluir hashtags relevantes

MÃ¡ximo 300 palavras, 3-4 parÃ¡grafos.
Estilo storytelling, nÃ£o lista."
```

### **Comandos Comuns**

```bash
# Ativar venv (Windows)
venv\Scripts\activate

# Ativar venv (Mac/Linux)
source venv/bin/activate

# Instalar dependÃªncias
pip install -r requirements.txt

# Formatar cÃ³digo (black)
black src/ app.py

# Lint (flake8)
flake8 src/ app.py

# Testes
pytest tests/

# Run Streamlit
streamlit run app.py

# Git workflow
git status
git add .
git commit -m "feat: add new feature"
git push origin main

# Create new branch
git checkout -b feature/new-feature

# Check logs
tail -f logs/app.log
```

---

## ðŸ“š RECURSOS E LINKS {#recursos}

### **Cursos Online (Gratuitos/Pagos)**

**IA Generativa**:
- [DeepLearning.AI - ChatGPT Prompt Engineering](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/) - FREE, 1h
- [DeepLearning.AI - LangChain for LLM Apps](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/) - FREE, 1h
- [DeepLearning.AI - Building Systems with ChatGPT](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/) - FREE, 1h

**RAG e Vector Databases**:
- [LangChain Documentation - RAG](https://python.langchain.com/docs/use_cases/question_answering/) - FREE
- [Pinecone Learning Center](https://www.pinecone.io/learn/) - FREE
- [ChromaDB Documentation](https://docs.trychroma.com/) - FREE

**MLOps**:
- [Made With ML - MLOps](https://madewithml.com/) - FREE
- [MLflow Tutorial](https://mlflow.org/docs/latest/tutorials-and-examples/index.html) - FREE
- [Great Expectations Docs](https://docs.greatexpectations.io/) - FREE

**dbt (Data Build Tool)**:
- [dbt Learn](https://learn.getdbt.com/) - FREE
- [dbt Fundamentals](https://courses.getdbt.com/courses/fundamentals) - FREE, 4h
- [dbt Advanced](https://courses.getdbt.com/courses/advanced-materializations) - FREE

**Modern Data Stack**:
- [Google Cloud Skills Boost](https://www.cloudskillsboost.google/) - FREE
- [BigQuery Fundamentals](https://cloud.google.com/bigquery/docs) - FREE
- [Azure Data Fundamentals](https://learn.microsoft.com/en-us/training/courses/dp-900t00) - FREE

**CertificaÃ§Ãµes**:
- [Microsoft AI-900](https://learn.microsoft.com/en-us/certifications/exams/ai-900/) - $99, 45 min
- [Google Cloud Data Engineer](https://cloud.google.com/certification/data-engineer) - $200
- [AWS Data Analytics](https://aws.amazon.com/certification/certified-data-analytics-specialty/) - $300

### **DocumentaÃ§Ã£o TÃ©cnica**

**Core Libraries**:
- [LangChain Python Docs](https://python.langchain.com/docs/)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Anthropic Claude API](https://docs.anthropic.com/)
- [SQLAlchemy 2.0](https://docs.sqlalchemy.org/en/20/)
- [Streamlit Docs](https://docs.streamlit.io/)

**Data Engineering**:
- [dbt Documentation](https://docs.getdbt.com/)
- [Apache Airflow](https://airflow.apache.org/docs/)
- [Great Expectations](https://docs.greatexpectations.io/)

**SQL Parsing**:
- [sqlglot Documentation](https://github.com/tobymao/sqlglot)
- [sqlparse Documentation](https://sqlparse.readthedocs.io/)

### **Comunidades e Networking**

**Slack/Discord**:
- [dbt Community Slack](https://www.getdbt.com/community/join-the-community/) - ~50k membros
- [Data Engineering Slack](https://dataengineering.wiki/) - ~40k membros
- [Locally Optimistic Slack](https://locallyoptimistic.com/community/) - Data leaders
- [AI Engineers Discord](https://discord.gg/ai-engineers)

**Telegram**:
- Data Engineering Brasil - Procurar no Telegram
- Python Brasil - Procurar no Telegram

**LinkedIn Groups**:
- Data Engineering Professionals
- Modern Data Stack
- dbt (data build tool) Community

**Reddit**:
- r/dataengineering
- r/datascience
- r/MachineLearning
- r/LocalLLaMA (open source LLMs)

### **Blogs e Newsletters**

**Blogs TÃ©cnicos**:
- [Locally Optimistic](https://locallyoptimistic.com/) - Data leadership
- [Seattle Data Guy](https://www.seattledataguy.com/) - Data engineering
- [Benn Stancil (Mode Analytics)](https://benn.substack.com/) - Analytics
- [Lenny's Newsletter](https://www.lennysnewsletter.com/) - Product + Data

**Company Blogs**:
- [dbt Blog](https://www.getdbt.com/blog/)
- [Databricks Blog](https://www.databricks.com/blog)
- [Snowflake Blog](https://www.snowflake.com/blog/)
- [Netflix Tech Blog](https://netflixtechblog.com/) - Data infra at scale
- [Uber Engineering](https://eng.uber.com/) - Data platforms

**Newsletters**:
- [Data Engineering Weekly](https://www.dataengineeringweekly.com/)
- [AI Breakfast](https://aibreakfast.beehiiv.com/) - AI news
- [The Sequence](https://thesequence.substack.com/) - AI/ML

### **GitHub Repositories Inspiradores**

**RAG Examples**:
- [langchain-tutorials](https://github.com/gkamradt/langchain-tutorials)
- [awesome-langchain](https://github.com/kyrolabs/awesome-langchain)
- [rag-from-scratch](https://github.com/anthropics/anthropic-cookbook/tree/main/skills/retrieval_augmented_generation)

**Data Engineering**:
- [awesome-data-engineering](https://github.com/igorbarinov/awesome-data-engineering)
- [data-engineer-roadmap](https://github.com/datastacktv/data-engineer-roadmap)
- [dbt-utils](https://github.com/dbt-labs/dbt-utils)

**MLOps**:
- [awesome-mlops](https://github.com/visenger/awesome-mlops)
- [ml-engineering](https://github.com/stas00/ml-engineering)

### **Ferramentas Ãšteis**

**Development**:
- [VS Code](https://code.visualstudio.com/) - IDE
- [Cursor Desktop](https://cursor.sh/) - AI-powered IDE
- [PyCharm](https://www.jetbrains.com/pycharm/) - Python IDE
- [DBeaver](https://dbeaver.io/) - Universal database tool

**Git/GitHub**:
- [GitHub Desktop](https://desktop.github.com/)
- [GitKraken](https://www.gitkraken.com/)
- [Sourcetree](https://www.sourcetreeapp.com/)

**Database**:
- [TablePlus](https://tableplus.com/) - Modern database GUI
- [pgAdmin](https://www.pgadmin.org/) - PostgreSQL GUI
- [MySQL Workbench](https://www.mysql.com/products/workbench/)

**Productivity**:
- [Notion](https://www.notion.so/) - Notes, docs, project management
- [Obsidian](https://obsidian.md/) - Markdown notes with graph
- [Todoist](https://todoist.com/) - Task management
- [Pomodoro Timer](https://pomofocus.io/) - Focus technique

**Screen Recording** (para demos):
- [OBS Studio](https://obsproject.com/) - FREE, pro-quality
- [Loom](https://www.loom.com/) - Easy browser recording
- [ScreenToGif](https://www.screentogif.com/) - GIF recording

### **Headhunters e Recrutadores**

**Lembrete**: Ver lista completa de 20 headhunters no `PLANO_COMPLETO_TRANSICAO_CARREIRA.md`

**Top 5 PrioritÃ¡rios**:
1. Renata Brito - Talento Incluir - linkedin.com/in/renataalvesb
2. Felipe Flores - Indicium (Founder) - linkedin.com/in/felipeflores
3. Carla Mendes - Accenture Data & AI - linkedin.com/in/carlamendes
4. Mariana Dias - Rocket HR - linkedin.com/in/marianadias-rh
5. Roberto Silva - Deloitte Analytics - linkedin.com/in/robertosilva

**Template de Mensagem**:
```
OlÃ¡ [Nome],

Vi seu trabalho com posiÃ§Ãµes em [Data/IA] e achei que faria sentido conversarmos.

Sou arquiteto de dados com 25 anos de experiÃªncia (TCU, TST, Sebrae), focando agora em Dados + IA Generativa.

Resumo rÃ¡pido:
â€¢ Especialista em DW/BI/GovernanÃ§a
â€¢ ExperiÃªncia com Ã³rgÃ£os pÃºblicos + compliance (LGPD)
â€¢ Aplicando IA para modernizaÃ§Ã£o de sistemas legados
â€¢ Stack: dbt, Python, LangChain, RAG, Vector DBs

Portfolio recente: [3 projetos + certificaÃ§Ã£o AI-900]

Buscando: Data Architect/Lead/CDO, Remoto/HÃ­brido BrasÃ­lia, R$ 18k-35k CLT ou R$ 250-400/h PJ

Teria 15 minutos para detalhar meu perfil?

Portfolio: [link GitHub]

Abs,
Paulo
```

### **Job Boards EspecÃ­ficas**

**Brasil**:
- [LinkedIn Jobs](https://www.linkedin.com/jobs/)
- [Glassdoor Brasil](https://www.glassdoor.com.br/)
- [Indeed Brasil](https://br.indeed.com/)
- [Trampos.co](https://trampos.co/) - Tech jobs
- [GeekHunter](https://www.geekhunter.com.br/) - Developers
- [Programathor](https://programathor.com.br/) - Remote tech

**Internacional (Remoto)**:
- [WeWorkRemotely](https://weworkremotely.com/)
- [RemoteOK](https://remoteok.com/)
- [Turing](https://www.turing.com/) - Remote for top devs
- [Toptal](https://www.toptal.com/) - Freelance (hard to get in)

**Data Specific**:
- [DataJobs.com](https://datajobs.com/)
- [AI Jobs](https://aijobs.net/)
- [MLOps Jobs](https://mlopsjobs.com/)

### **Freela Platforms**

**Brasil**:
- [Workana](https://www.workana.com/)
- [99Freelas](https://www.99freelas.com.br/)
- [GetNinjas](https://www.getninjas.com.br/)

**Internacional**:
- [Upwork](https://www.upwork.com/)
- [Fiverr Pro](https://www.fiverr.com/pro) (precisa aplicaÃ§Ã£o)
- [Toptal](https://www.toptal.com/) (elite, processo seletivo rigoroso)
- [Gun.io](https://gun.io/) (vetted freelancers)

---

## ðŸŽ¯ OBJETIVOS E MÃ‰TRICAS

### **Objetivo PrimÃ¡rio**

**Conseguir emprego/consultoria em 30-60 dias**

**Positions Target**:
- Senior Data Architect (R$ 25k-60k CLT)
- Lead Data Engineer (R$ 20k-40k CLT)
- Principal Data Engineer (R$ 30k-50k CLT)
- Head of Data Governance (R$ 25k-45k CLT)
- CDO (empresas mÃ©dias) (R$ 40k-80k CLT)
- Data Consultant (R$ 250-400/hora PJ)

**Locations**:
- **PrioritÃ¡rio**: Remoto 100%
- **AceitÃ¡vel**: HÃ­brido BrasÃ­lia-DF (1-2x/semana)
- **NÃ£o aceitÃ¡vel**: Presencial 5x/semana

### **MÃ©tricas de Sucesso - Semana 1**

**Portfolio**:
- [x] 3 projetos GitHub completos
- [x] 3 READMEs profissionais
- [x] 3 demos em vÃ­deo (2-3 min)
- [x] 1 artigo tÃ©cnico Medium
- [x] Portfolio website (GitHub Pages)

**Networking**:
- [ ] 25+ aplicaÃ§Ãµes de vagas
- [ ] 15+ headhunters contatados
- [ ] 5+ CTOs/Heads contatados
- [ ] 3+ conversas agendadas

**PresenÃ§a Online**:
- [x] 5+ posts LinkedIn
- [x] Perfil LinkedIn otimizado
- [x] #OpenToWork ativado
- [ ] 500+ impressÃµes posts LinkedIn
- [ ] 100+ visualizaÃ§Ãµes GitHub

**CertificaÃ§Ã£o**:
- [ ] Microsoft AI-900 obtido

### **MÃ©tricas de Sucesso - Semana 2**

**Pipeline**:
- [ ] 1-2 entrevistas tÃ©cnicas agendadas
- [ ] 3+ processos seletivos em andamento (2Âª/3Âª rodada)
- [ ] 1+ projeto freela em negociaÃ§Ã£o

**Inbound**:
- [ ] 3+ recrutadores entrando em contato
- [ ] 1+ empresa entrando em contato direto (via LinkedIn/GitHub)

**Networking**:
- [ ] PresenÃ§a ativa em 2+ comunidades (dbt Slack, Data Eng Brasil)
- [ ] 3+ conversas tÃ©cnicas com peers

### **MÃ©tricas de Sucesso - Semana 3-4**

**Ofertas**:
- [ ] 1+ oferta de emprego (CLT ou PJ)
- [ ] 2+ projetos freela fechados
- [ ] 3+ processos em fase final

**Ideal Outcome**:
- [ ] 2-3 ofertas para escolher
- [ ] PosiÃ§Ã£o alinhada com objetivos (remoto, salÃ¡rio, stack)
- [ ] ComeÃ§ar novo emprego em 30-60 dias

### **Red Flags** (Se nÃ£o acontecer em 2 semanas)

- Menos de 3 conversas com recrutadores â†’ **AÃ§Ã£o**: Intensificar networking, revisar LinkedIn
- Nenhuma entrevista tÃ©cnica â†’ **AÃ§Ã£o**: Aplicar para +50 vagas, ajustar portfolio
- Feedback negativo em entrevistas â†’ **AÃ§Ã£o**: Praticar mock interviews, revisar projetos

---

## ðŸ“ NOTAS FINAIS

### **Filosofia do Projeto**

**PrincÃ­pios Guia**:
1. **Funcionalidade > PerfeiÃ§Ã£o**: MVP funcionando > soluÃ§Ã£o elegante incompleta
2. **Portfolio > CurrÃ­culo**: Mostre, nÃ£o conte
3. **Networking > Job Boards**: Relacionamentos > aplicaÃ§Ãµes anÃ´nimas
4. **ConsistÃªncia > Intensidade**: 4 horas/dia consistentes > 12h um dia e 0h outro
5. **Aprenda fazendo**: Projetos reais > cursos passivos

**Mindset**:
- 25 anos de experiÃªncia nÃ£o se aprende em bootcamp (seu diferencial!)
- ExperiÃªncia com Ã³rgÃ£os pÃºblicos Ã© RARA (valorize isso!)
- VocÃª sabe o que pode dar errado (experiÃªncia = antecipaÃ§Ã£o de problemas)
- IA Ã© ferramenta, nÃ£o substituto (vocÃª + IA > IA sozinha)
- Mercado PRECISA de seniores que entendem IA (supply/demand a seu favor)

### **Quando Travar (Troubleshooting)**

**Se ficar travado >30 minutos em um problema:**

1. **Tentar 15 min sozinho**:
   - Ler documentaÃ§Ã£o
   - Revisar cÃ³digo linha por linha
   - Print/log debugging

2. **Google/Stack Overflow 15 min**:
   - Buscar erro exato
   - Ler issues no GitHub do projeto
   - Verificar versÃµes de dependÃªncias

3. **Perguntar ao Claude**:
   ```
   "Estou com erro X ao fazer Y.
   
   CÃ³digo:
   [colar cÃ³digo]
   
   Erro:
   [colar erro completo]
   
   O que tentei:
   - A
   - B
   - C
   
   Como resolver?"
   ```

4. **Se ainda travar: Simplificar**:
   - Remover features nÃ£o-essenciais
   - Fazer versÃ£o mais simples funcionando
   - Iterar depois

**NÃƒO perca mais de 30 minutos travado!**

### **TÃ©cnicas de Produtividade**

**Pomodoro** (altamente recomendado):
```
25 min trabalho focado (sem distraÃ§Ãµes!)
 5 min pausa (levantar, Ã¡gua, alongar)
25 min trabalho focado
 5 min pausa
25 min trabalho focado
 5 min pausa
25 min trabalho focado
15-30 min pausa longa
```

**Ferramentas**:
- [Pomofocus](https://pomofocus.io/)
- [Forest App](https://www.forestapp.cc/) (gamificaÃ§Ã£o)
- [Toggl Track](https://toggl.com/track/) (time tracking)

**Eliminar DistraÃ§Ãµes**:
- Modo aviÃ£o no celular
- Bloqueador de sites (Freedom, Cold Turkey)
- Headphones com mÃºsica instrumental/white noise
- Avisar famÃ­lia "estou trabalhando X-Y horas"

### **Cuidado com SaÃºde Mental**

**4 dias intensos = maratona, nÃ£o sprint**

**Sinais de Alerta**:
- Ansiedade excessiva ("preciso terminar TUDO hoje!")
- Perfeccionismo paralisante ("nÃ£o vou publicar atÃ© estar perfeito")
- Burnout ("nÃ£o aguento mais olhar cÃ³digo")

**Se sentir assim**:
1. **Pare. Respire. Descanse.**
2. MVP funcionando > projeto perfeito nÃ£o publicado
3. Progresso > perfeiÃ§Ã£o
4. Lembre: 4 dias Ã© MUITO para fazer portfolio que levaria 2-3 meses
5. VocÃª estÃ¡ indo bem! Celebre pequenas vitÃ³rias

**Balance**:
- Dormir 7-8h/noite (cÃ³digo com sono = bugs)
- Comer bem (cÃ©rebro precisa energia)
- ExercÃ­cio (mesmo 15 min caminhada)
- Pausas regulares (Pomodoro!)

---

## âœ… CHECKLIST FINAL

**Antes de ComeÃ§ar**:
- [ ] Ler este documento completamente
- [ ] Ler GUIA_PASSO_A_PASSO.md
- [ ] Ler PLANO_COMPLETO_TRANSICAO_CARREIRA.md
- [ ] Setup ambiente (Python, Git, OpenAI API)
- [ ] Estrutura de pastas criada
- [ ] LinkedIn atualizado + #OpenToWork
- [ ] Primeiro post publicado
- [ ] 5 headhunters contatados

**Durante Desenvolvimento**:
- [ ] Seguir cronograma dia por dia
- [ ] Usar Pomodoro (25min on, 5min off)
- [ ] Commitar cÃ³digo frequentemente
- [ ] Testar antes de seguir para prÃ³ximo mÃ³dulo
- [ ] NÃ£o travar >30min em problema (pedir ajuda!)

**ApÃ³s Cada Projeto**:
- [ ] CÃ³digo testado e funcionando
- [ ] README completo com badges
- [ ] Demo vÃ­deo gravado (2-3 min)
- [ ] Git push para GitHub
- [ ] Post LinkedIn publicado
- [ ] 5 aplicaÃ§Ãµes de vagas
- [ ] Celebrar! ðŸŽ‰

**Ao Final dos 4 Dias**:
- [ ] 3 projetos publicados
- [ ] Portfolio website live
- [ ] Artigo Medium publicado
- [ ] 25+ aplicaÃ§Ãµes enviadas
- [ ] 15+ headhunters contatados
- [ ] CertificaÃ§Ã£o AI-900 obtida
- [ ] Planejamento Semana 2 feito
- [ ] DESCANSAR! VocÃª merece! ðŸ¾

---

## ðŸš€ PRÃ“XIMOS PASSOS

**O QUE FAZER AGORA**:

1. **Salvar este documento** em `C:\projetos\paulo_sousa\paulo-career-accelerator\docs\CURSOR_MASTER_CONTEXT.md`

2. **Se usando Cursor Desktop**:
   - Abrir projeto em Cursor
   - Adicionar este arquivo ao contexto
   - Cursor agora tem TODO o contexto!

3. **Se usando outro IDE** (VS Code, PyCharm):
   - Manter este arquivo aberto para referÃªncia
   - Consultar sempre que precisar

4. **Abrir GUIA_PASSO_A_PASSO.md** e comeÃ§ar DIA 1!

5. **Quando precisar de ajuda**:
   - Voltar a este documento para consultar especificaÃ§Ãµes
   - Pedir ao Claude/Cursor: "Implementar X conforme CURSOR_MASTER_CONTEXT.md"
   - Sempre referenciar este documento para manter consistÃªncia

---

## ðŸ“ž SUPORTE

**Se precisar de ajuda**:

1. **Consultar DocumentaÃ§Ã£o**:
   - Este arquivo (contexto completo)
   - GUIA_PASSO_A_PASSO.md (tutorial literal)
   - PLANO_COMPLETO_TRANSICAO_CARREIRA.md (estratÃ©gia)

2. **Perguntar ao Claude** (aqui ou Cursor):
   ```
   "Seguindo CURSOR_MASTER_CONTEXT.md, preciso de ajuda com X..."
   ```

3. **Comunidades**:
   - dbt Slack (dÃºvidas dbt)
   - Data Engineering Brasil (dÃºvidas gerais)
   - Stack Overflow (cÃ³digo especÃ­fico)

---

## ðŸŽ¯ LEMBRETE FINAL

**VocÃª estÃ¡ fazendo algo incrÃ­vel!**

- 4 dias para portfolio completo (normalmente leva meses)
- 25 anos experiÃªncia + IA = combinaÃ§Ã£o poderosa
- Mercado PRECISA de profissionais assim
- Cada hora investida agora = potencial retorno enorme

**Mantra**:
> "Progresso, nÃ£o perfeiÃ§Ã£o. ConsistÃªncia, nÃ£o intensidade. Funcionalidade, nÃ£o elegÃ¢ncia. Portfolio, nÃ£o desculpas."

**VAMOS LÃ! ðŸš€**

---

**Ãšltima atualizaÃ§Ã£o**: Novembro 2024  
**VersÃ£o**: 1.0  
**Autor**: Paulo Cesar Mendes de Sousa Junior  
**Projeto**: paulo-career-accelerator
